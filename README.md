# crawler
learn how to write the python crawler


**网络爬虫根据使用场景分为通用爬虫和聚焦爬虫：**

1. 通用爬虫：搜索引擎用的爬虫系统

    - 目标：尽可能的把互联网上的所有网页下载下来，放到本地服务器里形成备份（百度快照），
        再对这些网页做相关处理（提取关键字、去掉广告），最后提供一个用户检索接口。
        
    - 抓取流程：
        1. 选取一部分已有的URL，把这些URL放入待爬取队列
        2. 从队列里取出这些URL，然后解析DNS得到主机IP，然后去这个IP对应的服务器里下载HTML页面，
            保存到搜索引擎的本地服务器
        3. 分析这些网页内容，找出网页里其他的URL链接，继续执行第二步，直到爬取条件结束
    
    - 搜索引擎如何获取一个新网站的URL：
        1. 主动向搜索引擎提交网址：http://zhanzhang.baidu.com/linksubmit/url
        2. 在其他网站设置网站的外链
        3. 搜索引擎会和DNS服务商进行合作，可以快速收录新的网站
        
    - 通用爬虫要遵守规则：Robots协议
    
    - 工作流程：爬取网页--存储数据--内容处理--提供检索/排名服务
    
    - 搜索引擎排名：
        1. PageRank值：根据网站的流量（点击量、浏览量、人气）统计，流量越高，排名越靠前
        2. 竞价排名：用钱来提高排名
        
    - 通用爬虫缺点：
        1. 只能提供和文本相关的内容，不能提供多媒体和二进制（程序、脚本）文件
        2. 提供结果千篇一律，不能针对不同背景领域的人提供不同的搜索结果
        3. 不能理解人类语义上的检索
        
2. 聚焦爬虫：
    - 爬虫程序员写的针对某种内容的爬虫
    - 面向主题、面向需求
